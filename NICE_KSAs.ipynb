{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Load employee dataset\n",
        "employee_data = pd.read_csv('/content/UpdatedResumeDataSet.csv')\n",
        "\n",
        "# Load KSA dataset\n",
        "ksa_data = pd.read_csv('/content/ksaNICE.tsv', delimiter='\\t')\n",
        "\n",
        "# Preprocess employee data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenize and remove stop words\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "employee_data['processed_resume'] = employee_data['Resume'].apply(preprocess_text)\n",
        "\n",
        "# Preprocess KSA data\n",
        "ksa_data['processed_ksa'] = ksa_data['Knowledge'].apply(preprocess_text)\n",
        "\n",
        "# Group KSA data by Role ID and aggregate KSA descriptions\n",
        "grouped_ksa_data = ksa_data.groupby('Role')['processed_ksa'].agg(lambda x: ' '.join(x)).reset_index()\n",
        "\n",
        "# Vectorize employee resumes and consolidated KSA descriptions\n",
        "vectorizer = TfidfVectorizer()\n",
        "employee_matrix = vectorizer.fit_transform(employee_data['processed_resume'])\n",
        "ksa_matrix = vectorizer.transform(grouped_ksa_data['processed_ksa'])\n",
        "\n",
        "# Calculate cosine similarity between employee resumes and consolidated KSA descriptions\n",
        "cosine_similarities = cosine_similarity(employee_matrix, ksa_matrix)\n",
        "\n",
        "# Find the most suited role for each employee\n",
        "employee_data['suited_role_id'] = cosine_similarities.argmax(axis=1)\n",
        "employee_data['suited_role_description'] = grouped_ksa_data.loc[employee_data['suited_role_id'], 'processed_ksa'].values\n",
        "\n",
        "# Display results\n",
        "print(employee_data[['Category', 'suited_role_id', 'suited_role_description']])\n",
        "\n",
        "employee_data[['Category', 'suited_role_id', 'suited_role_description']].to_csv('/content/suited_roles_results.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "aOyZxRc0MaaP",
        "outputId": "b3aa5e59-aeeb-4c14-84d2-97ee01a7968d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Category  suited_role_id  \\\n",
            "0    Data Science               1   \n",
            "1    Data Science               1   \n",
            "2    Data Science               1   \n",
            "3    Data Science               1   \n",
            "4    Data Science               1   \n",
            "..            ...             ...   \n",
            "957       Testing              13   \n",
            "958       Testing              13   \n",
            "959       Testing              13   \n",
            "960       Testing              12   \n",
            "961       Testing              13   \n",
            "\n",
            "                               suited_role_description  \n",
            "0    knowledge computer networking concepts protoco...  \n",
            "1    knowledge computer networking concepts protoco...  \n",
            "2    knowledge computer networking concepts protoco...  \n",
            "3    knowledge computer networking concepts protoco...  \n",
            "4    knowledge computer networking concepts protoco...  \n",
            "..                                                 ...  \n",
            "957  knowledge risk management processes methods as...  \n",
            "958  knowledge risk management processes methods as...  \n",
            "959  knowledge risk management processes methods as...  \n",
            "960  knowledge computer networking concepts protoco...  \n",
            "961  knowledge risk management processes methods as...  \n",
            "\n",
            "[962 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Load datasets\n",
        "employee_data = pd.read_csv('/content/UpdatedResumeDataSet.csv')\n",
        "ksa_data = pd.read_csv('/content/ksaNICE.tsv', delimiter='\\t')\n",
        "\n",
        "# Preprocess employee data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenize and remove stop words\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "employee_data['processed_resume'] = employee_data['Resume'].apply(preprocess_text)\n",
        "\n",
        "# Preprocess KSA data\n",
        "ksa_data['processed_ksa'] = ksa_data['Knowledge'].apply(preprocess_text)\n",
        "\n",
        "# Group KSA data by Role ID and aggregate KSA descriptions\n",
        "grouped_ksa_data = ksa_data.groupby('Role')['processed_ksa'].agg(lambda x: ' '.join(x)).reset_index()\n",
        "\n",
        "# Vectorize employee resumes and consolidated KSA descriptions\n",
        "vectorizer = TfidfVectorizer()\n",
        "employee_matrix = vectorizer.fit_transform(employee_data['processed_resume'])\n",
        "ksa_matrix = vectorizer.transform(grouped_ksa_data['processed_ksa'])\n",
        "\n",
        "# Calculate cosine similarity between employee resumes and consolidated KSA descriptions\n",
        "cosine_similarities = cosine_similarity(employee_matrix, ksa_matrix)\n",
        "\n",
        "# Find the most suited role for each employee\n",
        "employee_data['suited_role_id'] = cosine_similarities.argmax(axis=1)\n",
        "employee_data['suited_role_description'] = grouped_ksa_data.loc[employee_data['suited_role_id'], 'processed_ksa'].values\n",
        "employee_data['suited_role_id'] = grouped_ksa_data.loc[employee_data['suited_role_id'], 'Role'].values\n",
        "\n",
        "# Save results to a CSV file\n",
        "output_file_path = '/content/suited_roles_results.csv'\n",
        "employee_data[['Category', 'suited_role_id', 'suited_role_description']].to_csv(output_file_path, index=False)\n",
        "\n",
        "# Display confirmation message\n",
        "print(f\"Results saved to: {output_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "wwkQDZ6YNhwL",
        "outputId": "ff5aed47-dce4-4250-a71e-e8462e594573"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to: /content/suited_roles_results.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Load datasets\n",
        "employee_data = pd.read_csv('/content/UpdatedResumeDataSet.csv')\n",
        "ksa_data = pd.read_csv('/content/ksaNICE.tsv', delimiter='\\t')\n",
        "\n",
        "# Preprocess employee data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenize and remove stop words\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "# Fill NaN values with an empty string in 'Resume' column\n",
        "employee_data['Resume'] = employee_data['Resume'].fillna('')\n",
        "\n",
        "# Preprocess 'Resume' column\n",
        "employee_data['processed_resume'] = employee_data['Resume'].apply(preprocess_text)\n",
        "\n",
        "# Fill NaN values with an empty string in 'Knowledge' column\n",
        "ksa_data['Knowledge'] = ksa_data['Knowledge'].fillna('')\n",
        "\n",
        "# Preprocess 'Knowledge' column\n",
        "ksa_data['processed_ksa'] = ksa_data['Knowledge'].apply(preprocess_text)\n",
        "\n",
        "# Prepare features and labels\n",
        "X = employee_data['processed_resume'] + ' ' + ksa_data['processed_ksa']\n",
        "y = employee_data['Category']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize the textual data\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "# Train a Support Vector Machine classifier\n",
        "clf = SVC(kernel='linear', random_state=42)\n",
        "clf.fit(X_train_vectorized, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = clf.predict(X_test_vectorized)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n",
        "\n",
        "# Save results to a CSV file\n",
        "output_file_path = '/content/classification_results.csv'\n",
        "results = pd.DataFrame({'Actual': y_test, 'Predicted': predictions})\n",
        "results.to_csv(output_file_path, index=False)\n",
        "\n",
        "# Display confirmation message\n",
        "print(f\"Results saved to: {output_file_path}\")\n"
      ],
      "metadata": {
        "id": "o-iGFVk-PDlT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}